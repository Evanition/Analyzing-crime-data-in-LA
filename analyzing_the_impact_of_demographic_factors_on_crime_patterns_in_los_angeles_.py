# -*- coding: utf-8 -*-
"""Analyzing the Impact of Demographic Factors on Crime Patterns in Los Angeles: .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R_uJ0c9QVrfrpsB5Z_KMN_QAQFzXvACH

**Analyzing the Impact of Demographic Factors on Crime Patterns in Los Angeles** \\
Dewayne Barnes, Caia Gelli, Jocelyn Niemiec

We decided to do all imports at the top of the notebook, for the sake of clarity.
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.cluster import KMeans
import seaborn as sns
from IPython.display import display, FileLink
from sklearn.neighbors import KernelDensity

!pip install sqlalchemy==1.4.46
!pip install pandasql

from pandasql import sqldf

# mount google drive
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/545/Crime_Data_from_2020_to_Present.csv")
# my csv is in a file called 545 under my drive
# https://catalog.data.gov/dataset/crime-data-from-2020-to-present

from google.colab import drive
drive.mount('/content/drive')

# import demographics dataset
# demograph_df = pd.read_csv("/content/drive/MyDrive/545/")

"""# Introduction/Background
Goal: analyze what demographic factors make a victim susceptible types of crime being committed against them. Which populations are most vulnerable etc. etc.

overview of dataset

- link the handbook

EDA
- Crime distribution by area and crime type
- Time series plot of crime incidents
- top list of crimes (horizontal bar graph)
- number of crimes listed with secondary/tertiary crime codes (vert bar) as percentage
- time of day buckets to see when crimes are most commonly committed (bar graph)
- count of victim descent
- distribution of victims ages with box plot - redo box plot
- - NOTE: victims age was analyzed twice (second time we dropped the zeros) - found out how cars are counted as 0 years old

First query - top ten list of crimes
-stolen vehicle is by far the most popular crime. The second most prominent is simple assault.
- assault via intimate partner
-- curious about domestic and sexual violence (interested in possible correlation)
- number of crimes committed per year - basic commentary on how 2024 is incomplete and how covid era had less crimes
- date reported vs date occuredd - overwhelmingly linear relationship - few outliers so we won't investigate too much
- crime heatmap - might drop - but in essence most of the crime occurs in the middle of the region (reported crime)

# EDA

**Crime distribution by area and crime type** \\
Visualizes the distribution of crime incidents across various areas. Each slice represents a different area, and the percentage indicates its contribution to the total number of crimes.
"""

# Aggregate the data to count the number of crimes in each area
crime_distribution = df['AREA NAME'].value_counts()

# Plot the pie chart
plt.figure(figsize=(10, 8))
crime_distribution.plot(kind='pie', autopct='%1.1f%%', startangle=140)
plt.title('Crime Distribution by Area')
plt.ylabel('')
plt.show()

# Aggregate the data to count the number of occurrences of each crime type in each area
crime_distribution_by_area = df.groupby(['AREA NAME', 'Crm Cd']).size().unstack()

# Plot the stacked bar plot
plt.figure(figsize=(12, 8))
crime_distribution_by_area.plot(kind='bar', stacked=True)
plt.title('Crime Distribution by Area and Crime Type')
plt.xlabel('Area')
plt.ylabel('Number of Incidents')
plt.legend(title='Crime Type', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""**Time series plot of crime incidents** \\
This code performs analysis on a dataset containing crime incidents. It first converts the 'Date Occurred' column to datetime format if it's not already, extracts year and month from it, and then aggregates the data to count the number of crime incidents by year and month. Finally, it visualizes the trend of crime incidents over time using a time series plot.
"""

# Convert the 'Date Occurred' column to datetime format if it's not already
df['DATE OCC'] = pd.to_datetime(df['DATE OCC'])

# Extract year and month from the 'Date Occurred' column
df['Year'] = df['DATE OCC'].dt.year
df['Month'] = df['DATE OCC'].dt.month

# Aggregate the data to count the number of crime incidents by year and month
crime_incidents_over_time = df.groupby(['Year', 'Month']).size()

# Plot the time series
plt.figure(figsize=(12, 6))
crime_incidents_over_time.plot()
plt.title('Crime Incidents Over Time')
plt.xlabel('Year-Month')
plt.ylabel('Number of Incidents')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""**SQL Query** \\
The SQL query selects columns "Crm Cd" and "Crm Cd Desc" from the DataFrame df.
It groups the data by "Crm Cd" and "Crm Cd Desc" and counts the occurrences of each crime code.
"""

query = """
SELECT df."Crm Cd" AS Crm_Cd, df."Crm Cd Desc" AS Crm_Cd_Desc, COUNT(df."Crm Cd") AS count
FROM df
GROUP BY df."Crm Cd", df."Crm Cd Desc"
ORDER BY count DESC
"""

# Execute the query using sqldf function
count_df = sqldf(query, locals())

# Display the resulting dataframe
print(count_df.head(10))

plt.figure(figsize=(12, 8))
sns.barplot(x='count', y='Crm_Cd_Desc', data=count_df.head(10), palette='plasma')
plt.xlabel('Count')
plt.ylabel('Crime Description')
plt.title('Top 10 Crime Types by Count')
plt.show()

"""**Calculating total number of records and sorting that with crime codes** \\
This code segment examines a dataset of crime records to determine the prevalence of multiple crime codes within each entry. By calculating the percentages of records with 2, 3, and 4 crime codes relative to the total number of records, it offers insights into the distribution of multi-coded crime incidents, visualized through a bar plot. \\

Notably only 7% of crimes have 2 crime codes. The use of 3rd and 4th crime codes is less than 1%. Thus we concluded that secondary crime codes would not be of much importance in comparison to the primary crime codes. \\

"""

# Calculate total number of records
total_query = """
SELECT COUNT(*) AS total
FROM df
"""
total_df = sqldf(total_query, locals())
total_records = total_df['total'][0]

# Calculate count of records with 2 crime codes
count_2_query = """
SELECT COUNT(*) AS count_2
FROM df
WHERE "Crm Cd" IS NOT NULL AND "Crm Cd 2" IS NOT NULL;
"""
count_2_df = sqldf(count_2_query, locals())
count_2 = count_2_df['count_2'][0]

# Calculate count of records with 3 crime codes
count_3_query = """
SELECT COUNT(*) AS count_3
FROM df
WHERE "Crm Cd" IS NOT NULL AND "Crm Cd 2" IS NOT NULL AND "Crm Cd 3" IS NOT NULL;
"""
count_3_df = sqldf(count_3_query, locals())
count_3 = count_3_df['count_3'][0]

# Calculate count of records with 4 crime codes
count_4_query = """
SELECT COUNT(*) AS count_4
FROM df
WHERE "Crm Cd" IS NOT NULL AND "Crm Cd 2" IS NOT NULL AND "Crm Cd 3" IS NOT NULL AND "Crm Cd 4" IS NOT NULL;
"""
count_4_df = sqldf(count_4_query, locals())
count_4 = count_4_df['count_4'][0]

# Calculate the percentages
percent_2_codes = (count_2 / total_records) * 100
percent_3_codes = (count_3 / total_records) * 100
percent_4_codes = (count_4 / total_records) * 100

# Combine the results into a DataFrame
percentages_df = pd.DataFrame({
    'Category': ['2 Codes', '3 Codes', '4 Codes'],
    'Percentage': [percent_2_codes, percent_3_codes, percent_4_codes]
})

# Plot results
sns.set(style="whitegrid")
plt.figure(figsize=(10, 6))
sns.barplot(x='Category', y='Percentage', data=percentages_df, palette='coolwarm')
plt.xlabel('Crime Code Combination')
plt.ylabel('Percentage of Records')
plt.title('Percentage of Records with Multiple Crime Codes')
plt.show()

# most crimes only have their primary crime code so crm cd 2, 3, 4 won't be as important in our invesitgation
percentages_df

"""We created 4 time buckets for time of crime occurence to paint a clear picture of when most crimes are committed. We are able to see that most crime is committed in late afternoon/evening."""

def categorize_time_bucket(time_occurred):
    if 0 <= time_occurred < 400:
        return '00:00-03:59'
    elif 400 <= time_occurred < 800:
        return '04:00-07:59'
    elif 800 <= time_occurred < 1200:
        return '08:00-11:59'
    elif 1200 <= time_occurred < 1600:
        return '12:00-15:59'
    elif 1600 <= time_occurred < 2000:
        return '16:00-19:59'
    else:
        return '20:00-23:59'

df['Time_Block'] = df['TIME OCC'].apply(categorize_time_bucket)

query = """
SELECT
    df.Time_Block, COUNT("Crm Cd") AS count_crimes
FROM df
GROUP BY df.Time_Block
ORDER BY df.Time_Block;
"""

crime_time_df = sqldf(query, locals())

crime_time_df
plt.figure(figsize=(12, 8))
sns.barplot(x='Time_Block', y='count_crimes', data=crime_time_df, palette='rocket')

plt.xlabel('Time of Day')
plt.ylabel('Number of Crimes')
plt.title('Number of Crimes in 4-Hour Time Blocks')

plt.show()

"""In this section of the notebook, we focus on counting the occurrences of each victim descent category within our crime dataset. To ensure accuracy and consistency, we reference the symbol translation from the UCR (Uniform Crime Reporting) Handbook. This ensures that our analysis aligns with standard reporting practices. Additionally, we aim to contextualize our findings by comparing them to the demographics of Los Angeles (LA), which we obtain from DataUSA. According to DataUSA's 2021 statistics, the population demographics of LA are approximately as follows: 48.45% Hispanic, 11.6% Asian, 28.1% White, 8.31% Black, and 16.8% White Hispanic. By juxtaposing our dataset's victim descent frequencies with LA's population demographics, we can identify potential disparities or alignments, providing valuable insights into the representation of various demographic groups within crime victimization. \\



Link to demograpics of LA: \\
https://datausa.io/profile/geo/los-angeles-ca#:~:text=1.89M%20people-,In%202021%2C%20there%20were%201.19%20times%20more%20White%20(Non%2D,hispanic%20(1.89M%20people). \\

"""

query = """
SELECT df."Vict Descent", COUNT(df."Vict Descent") AS count
FROM df
GROUP BY df."Vict Descent"
ORDER BY count DESC
"""
descent_labels = {
    'A': 'Other Asian',
    'B': 'Black',
    'C': 'Chinese',
    'D': 'Cambodian',
    'F': 'Filipino',
    'G': 'Guamanian',
    'H': 'Hispanic/Latin/Mexican',
    'I': 'American Indian/Alaskan Native',
    'J': 'Japanese',
    'K': 'Korean',
    'L': 'Laotian',
    'O': 'Other',
    'P': 'Pacific Islander',
    'S': 'Samoan',
    'U': 'Hawaiian',
    'V': 'Vietnamese',
    'W': 'White',
    'X': 'Unknown',
    'Z': 'Asian Indian'
}

victim_descent_counts = sqldf(query, locals())

victim_descent_counts['Vict Descent Label'] = victim_descent_counts['Vict Descent'].map(descent_labels)

print(victim_descent_counts.head(19))

"""This section visualizes the distribution of victim ages in the dataset. To address data integrity issues, entries with "victimless" crimes or property crimes with ages listed as zero were filtered out. The resulting visualization, such as a box plot, provides insights into the age distribution of crime victims, excluding erroneous entries for accurate analysis."""

zero_df = df[df['Vict Age'] >= 0]

summary_statistics = zero_df['Vict Age'].describe()
print(summary_statistics)
mode = zero_df['Vict Age'].mode()
print(f'Mode of Vict Age: {mode}')
variance = zero_df['Vict Age'].var()
print(f'Variance of Vict Age: {variance}')

sns.set(style='whitegrid')
plt.figure(figsize=(10, 6))
sns.boxplot(x=zero_df['Vict Age'], orient='h', palette='pastel')

#df.boxplot(column='Vict Age', vert=False)
plt.ylabel('Vict Age')
plt.title('Box Plot of Vict Age')
plt.show()

# also note that a lot of the 0 ages are from unknown victims or property (like cars) - which is why we arent filtering them out yet

"""Here we focus on the visual distribution of victim age after removing entries with zeros. The observations indicate a notable increase in the average age and a more concentrated spread of data, particularly between the ages of 25 and 50. Additionally, outliers, such as extremely old individuals, are observed, with a singular entry indicating a 120-year-old, likely a data entry error. Despite this outlier, the overall analysis remains valid, as it provides valuable insights into the age demographics of crime victims within the dataset."""

no_zero_df = df[df['Vict Age'] > 0]

summary_statistics = no_zero_df['Vict Age'].describe()
print(summary_statistics)
mode = no_zero_df['Vict Age'].mode()
print(f'Mode of Vict Age: {mode}')
variance = no_zero_df['Vict Age'].var()
print(f'Variance of Vict Age: {variance}')

sns.set(style='whitegrid')
plt.figure(figsize=(10, 6))
sns.boxplot(x=no_zero_df['Vict Age'], orient='h', palette='pastel')

plt.ylabel('Vict Age')
plt.title('Box Plot of Vict Age')
plt.show()

"""With our visualizations of the number of crimes that occured by year, it provides commentary on the observed trends, noting a decrease in crime in 2020, likely attributed to the COVID-19 pandemic, followed by a subsequent rise in crime. Additionally, it calculates the mean including and excluding data from 2024 for a comprehensive understanding of the crime trend."""

df['Year_Occurred'] = pd.to_datetime(df['DATE OCC'])

df['Year_Occurred'] = df['Year_Occurred'].dt.year

query = """
SELECT
    Year_Occurred,
    COUNT("Crm Cd") AS count_occurrences
FROM
    df
GROUP BY
    Year_Occurred
ORDER BY
    Year_Occurred;
"""

year_counts = sqldf(query, locals())
print(year_counts)

yearly_mean = df.groupby('Year_Occurred')['Crm Cd'].count().mean()

yearly_counts = df.groupby('Year_Occurred')['Crm Cd'].count().reset_index()
yearly_counts.columns = ['Year', 'Number of Crimes']
yearly_mean = yearly_counts['Number of Crimes'].mean()

filtered_df = yearly_counts[yearly_counts['Year'] != 2024]

filtered_df.columns = ['Year', 'Number of Crimes']
yearly_mean_not_2024 = filtered_df['Number of Crimes'].mean()

sns.set(style="whitegrid")

plt.figure(figsize=(10, 6))
sns.barplot(x='Year', y='Number of Crimes', data=yearly_counts, hue='Year', palette='viridis')

plt.axhline(y=yearly_mean, color='red', linestyle='--', label=f'Mean (all years): {yearly_mean:.2f}')
plt.axhline(y=yearly_mean_not_2024, color='green', linestyle='--', label=f'Mean (not 2024): {yearly_mean_not_2024:.2f}')

plt.xlabel('Year')
plt.ylabel('Number of Crimes')
plt.title('Number of Crimes Occurred per Year')

plt.legend()

plt.show()

"""To analyze the relationship between the date crimes occurred and the date they were reported, we sampled 1000 points from the dataset due to its large size. The scatter plot revealed a mostly linear relationship, indicating that report times closely align with occurrence times. Given this close correspondence, further exploration of this characteristic was deemed unnecessary."""

sample_df = df.sample(n=1000, random_state=42)  # Adjust the sample size as needed - bigger samples crash the dataset

sample_df['DATE OCC'] = pd.to_datetime(sample_df['DATE OCC'])
sample_df['Date Rptd'] = pd.to_datetime(sample_df['Date Rptd'])

plt.figure(figsize=(10, 8))
plt.scatter(sample_df['DATE OCC'], sample_df['Date Rptd'], alpha=0.5, s=10)

plt.xlabel('Date Occurred')
plt.ylabel('Date Reported')
plt.title('Scatter Plot between Date Occurred and Date Reported')

plt.show()

"""**Heat map of crime using lat and long** \\
To visualize the geographic distribution of crime incidents, a kernel density estimation (KDE) plot was generated using a sample of 1000 points from the dataset. The plot depicts the density of crime occurrences across latitude and longitude coordinates. Warmer colors represent higher densities of crime incidents, while cooler colors indicate lower densities. The plot provides insights into areas with higher concentrations of crime, aiding in spatial analysis and resource allocation for law enforcement efforts.
"""

sample_size = 1000
sample_df = df.sample(n=sample_size, random_state=42)
sample_df = sample_df.dropna(subset=['LAT', 'LON'])

# Set plot size
plt.figure(figsize=(10, 8))

ax = sns.kdeplot(data=sample_df, x='LON', y='LAT', cmap='coolwarm', fill=True, thresh=0.05)

ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')
ax.set_title('Crime Heatmap with Latitude and Longitude (Sampled Data)')


ax.set_xlim(-125, -110)
ax.set_ylim(32, 36)

plt.show()

"""
This code snippet retrieves the data types of all columns in the DataFrame df. It then creates a new DataFrame data_types_df to store the column names along with their corresponding data types. This information is useful for understanding the structure of the dataset and the types of data stored in each column."""

data_types = df.dtypes

data_types_df = pd.DataFrame(data_types, columns=['DataType'])
data_types_df.reset_index(inplace=True)
data_types_df.rename(columns={'index': 'Column'}, inplace=True)

print(data_types_df)

# dataset
# https://data.lacity.org/Public-Safety/Crime-Data-from-2020-to-Present/2nrs-mtv8/about_data

"""# Data Preparation

This section of the data preparation process involved several steps:

1. Grouping races into buckets to match the broad categories defined for white and black demographics.
2. Creating age groups to categorize individuals based on their age ranges.
3. Encoding categorical variables such as sex and race into integers for modeling purposes.
4. Creating a list of rape codes to identify instances related to this specific crime type.
5. Distinguishing between property crimes and violent crimes based on guidelines outlined in the handbook, providing a clear separation between these two main categories for easier analysis and comparison.
6. Utilizing numerical representations for certain variables to facilitate analysis and model interpretation, considering the potential ease of analysis with numerical data for machine learning models.
"""

# maps to encode stuff
sex_mapping = {'M': 0, 'F': 1}

# 0 - white, 1 - black, 2 - asian, 3 - hispanic, 4 - native american, 5 - pacific islander/samoan, 18 - other/unkown
descent_mapping = {'A': 2, 'B': 1, 'C' : 2, 'D': 2, 'F': 2, 'G' : 3, 'H': 3, 'I': 4, 'J' : 2, 'K': 2, 'L': 2, 'O' : 18,
                   'P': 5, 'S': 5, 'U' : 5, 'V': 2, 'W': 0, 'Z' : 2, 'X' : 18  }

df['Age Category'] = pd.cut(df['Vict Age'], bins=[0, 18, 30, 50, np.inf], labels=['0-18', '19-30', '31-50', '51+'])


age_mapping = {'0-18' : 0, '19-30' : 1, '31-50' : 2, '51+' : 3}
code_mapping = {236 : 0, 626 : 1}

rape_codes = [121, 122, 815, 820, 821]

property_and_violent_crimes = [310, 320, 510, 520, 433, 330, 331, 410, 420, 421, 350, 351, 352, 353, 450, 451, 452, 453, 341, 343, 345,
 440, 441, 442, 443, 444, 445, 470, 471, 472, 473, 474, 475, 480, 485, 487, 491, 110, 113, 121, 122, 815, 820, 821, 210, 220, 230, 231, 235, 236, 250, 251, 761, 926,
 435, 436, 437, 622, 623, 624, 625, 626, 627, 647, 763, 928, 930]

violent_crimes = [110, 113, 121, 122, 815, 820, 821, 210, 220, 230, 231, 235, 236, 250, 251, 761, 926,
 435, 436, 437, 622, 623, 624, 625, 626, 627, 647, 763, 928, 930]

property_crimes = [310, 320, 510, 520, 433, 330, 331, 410, 420, 421, 350, 351, 352, 353, 450, 451, 452, 453, 341, 343, 345,
 440, 441, 442, 443, 444, 445, 470, 471, 472, 473, 474, 475, 480, 485, 487, 491]

"""made violent crimes df

"""

# violent crime codes as identified by UCR reporting crime codes handbook
violent_crimes_df = df[df['Crm Cd'].isin(violent_crimes)]
violent_crimes_df = violent_crimes_df.reset_index(drop=True)
violent_crimes_df

"""property crimes df"""

# df of property crimes

property_crimes_df = df[df['Crm Cd'].isin(property_crimes)]
property_crimes_df = property_crimes_df.reset_index(drop=True)
property_crimes_df

"""# Data Analysis

1. All Crimes vs. Violent Crimes (Low Accuracy): Initially, the analysis focuses on distinguishing between all crimes and violent crimes, which yielded low accuracy. To improve specificity, the analysis delves deeper into specific crime categories.
2. Factors Influencing Likelihood of Sexual Crimes: Utilizing logistic regression, the analysis aims to identify factors influencing the likelihood of becoming a victim of sexual crimes within the subset of violent crimes. Features like age are highlighted through coefficient analysis. Additionally, a random forest model is employed for comparative analysis of feature importance, assessing accuracy and mean squared error (MSE).
3. Demographics for Crimes of Domestic Violence: Recognizing the higher likelihood of women being victims of domestic violence, the analysis investigates demographic correlates such as age category and victim descent using logistic regression. Emphasis is placed on analyzing feature coefficients to understand the significance of age categories in determining the victim's sex.
4. Age Category and Domestic Violence Victims: Given the significance of age categories in victimization, the analysis examines the relationship between age categories and the status of domestic violence crimes (solved or unsolved). Both random forest and logistic regression models are utilized to assess feature importance and coefficients, particularly focusing on victim descent.
5. Property Crimes vs. Solved: Employing random forest analysis, the study explores important features in property crimes and their resolution status. Logistic regression is utilized to analyze the relationship between race and property crimes, with insights drawn from the random forest results.
6. Location vs. Crime: K-means clustering is employed to investigate the relationship between crime incidents and geographical locations, aiming to identify clusters or patterns in crime distribution across different regions.
7. Location vs. Vehicle Theft: With vehicle theft identified as the most prevalent crime, the analysis goes deeper into regions with the highest occurrences of vehicle theft. K-means clustering is utilized to explore demographic differences between low and high-crime regions within this context.

Creates a modified dataframe with an additional column indicating whether each crime is violent or not. Filters out rows with unknown or other values for sex and victim descent to ensure cleaner research. \\
violence victim df - original df but with column "violent crime" which is 1 if crime was violent and 0 otherwise \\
filters out unknown/other for sex (for cleaner research as it is a non homogenous category) and also unknown/other victim descents
"""

# violence_df for looking at victims of violent crimes
violence_victim_df = df.copy()
# only keeping victim demographics and crime
violence_victim_df = violence_victim_df[['Age Category', 'Vict Sex', 'Vict Descent', 'Crm Cd', 'Crm Cd Desc']].reset_index(drop=True)
violence_victim_df = violence_victim_df[violence_victim_df['Crm Cd'].isin(property_and_violent_crimes)]

# violent crime = 1 if the crime was violene
violence_victim_df['violent crime'] = violence_victim_df['Crm Cd'].isin(violent_crimes).astype(int)

# cleaning and setup for vict sex and descent
violence_victim_df['Vict Sex'] = violence_victim_df['Vict Sex'].map(sex_mapping)
violence_victim_df['Vict Sex'] = violence_victim_df['Vict Sex'].where(violence_victim_df['Vict Sex'].isin([0, 1]), np.nan)
violence_victim_df.dropna(subset=['Vict Sex'], inplace=True)

violence_victim_df['Vict Descent'] = violence_victim_df['Vict Descent'].map(descent_mapping)
violence_victim_df = violence_victim_df[violence_victim_df['Vict Descent'] != 18]
violence_victim_df.dropna(subset=['Vict Descent'], inplace=True)
violence_victim_df.reset_index(drop=True)
violence_victim_df

"""Sets up logistic regression to analyze the correlation between violent crime and age category, victim sex, and victim descent. Computes the accuracy of the logistic regression model and performs coefficient analysis to determine the impact of each feature on the prediction of violent crime. \\"""

# features
X = violence_victim_df[['Age Category', 'Vict Sex', 'Vict Descent']]
# target variable
y = violence_victim_df['violent crime']

X = pd.get_dummies(X, columns=['Age Category', 'Vict Sex', 'Vict Descent'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

logreg1 = LogisticRegression()
logreg1.fit(X_train, y_train)

y_pred = logreg1.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

coefficients = logreg1.coef_[0]

feature_names = X.columns
feature_coefficients1 = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})
print(feature_coefficients1)

# Plot the feature coefficients
# lower accuracy
plt.figure(figsize=(10, 6))
plt.barh(feature_coefficients1['Feature'], feature_coefficients1['Coefficient'], color='skyblue')
plt.xlabel('Coefficient Value')
plt.ylabel('Feature')
plt.title('Victim Demographics and Violent Crime')
plt.grid(axis='x')
plt.show()

"""because of the low accuracy - decided to make investigation a bit more specific. Using the violent crimes df, what demographic makes someone more likely to be victim of sexual assault. sexual crime column as int 0/1.

"""

# from violent crimes df - what demographic features make someone more likely to be a victim of a sexual crime
assault_victim_df = violent_crimes_df.copy()
assault_victim_df['sexual crime'] = assault_victim_df['Crm Cd'].isin(rape_codes).astype(int)

# drop unknown sex
assault_victim_df['Vict Sex'] = assault_victim_df['Vict Sex'].map(sex_mapping)
assault_victim_df['Vict Sex'] = assault_victim_df['Vict Sex'].where(assault_victim_df['Vict Sex'].isin([0, 1]), np.nan)
assault_victim_df.dropna(subset=['Vict Sex'], inplace=True)

# drop unknown descent
assault_victim_df['Vict Descent'] = assault_victim_df['Vict Descent'].map(descent_mapping)
assault_victim_df = assault_victim_df[assault_victim_df['Vict Descent'] != 18]
assault_victim_df.dropna(subset=['Vict Descent'], inplace=True)

assault_victim_df.reset_index(drop=True)

"""running log reg w/ target of sexual crime and features as age, sex, descent
accuracy of 0.97 \\
highest feature coeffs: 0-18, vict sex 1 (women)
"""

# features & target
X = assault_victim_df[['Age Category', 'Vict Sex', 'Vict Descent']]
y = assault_victim_df['sexual crime']

X = pd.get_dummies(X, columns=['Age Category', 'Vict Sex', 'Vict Descent'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

logreg = LogisticRegression(max_iter=300)
logreg.fit(X_train, y_train)

# accuracy
y_pred = logreg.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# feature coefficients
coefficients = logreg.coef_[0]

feature_names = X.columns
feature_coefficients = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})
print(feature_coefficients)


# Plot the feature coefficients
plt.figure(figsize=(10, 6))
plt.barh(feature_coefficients['Feature'], feature_coefficients['Coefficient'], color='skyblue')
plt.xlabel('Coefficient Value')
plt.ylabel('Feature')
plt.title('Sexual Assault and Violent Crime')
plt.show()

"""decided to run a random tree forest on the same variables to see if we find corroborating results. (we did) \\
rf has bigger diff b/w men vs women i think \\
random forest is good at showing which features are most important? for future steps prob gonna do rf and then log rg to do more specific exploration within feature categories \\
"""

# run same variables on random forest to see if we get similar results
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

feature_importances = rf_model.feature_importances_

plt.figure(figsize=(10, 6))
plt.bar(range(len(feature_importances)), feature_importances, tick_label=X.columns)
plt.xlabel('Feature')
plt.ylabel('Feature Importance')
plt.title('Sexual Assault and Violent Crime  (Random Forest Model)')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""**domestic violence df \\ 236, 626 \\** \\
Here we created a dataframe for domestic violence analysis, focusing on the correlation between victim sex and victim demographics such as descent and age category. The function filters out entries where the victim sex is known (i.e., not unknown or other) and sets up the target variable as victim sex (woman = 1, man = 0). It then uses random forest regression to analyze the importance of demographic features in predicting victim sex. It uses random forest with mse of 0.17 to look at age over descent.



"""

# looking at codes 236 and 626 (domestic violence)
domestic_df = df.copy()
domestic_df

domestic_df = domestic_df[(domestic_df['Crm Cd'].isin([236, 626]))]
domestic_df.drop(columns=['Crm Cd 1','Crm Cd 2', 'Crm Cd 3', 'Crm Cd 4', 'Status', 'Crm Cd Desc', 'DR_NO', 'AREA', 'AREA NAME', 'Weapon Desc',
                               'Weapon Used Cd', 'Premis Cd', 'Premis Desc', 'Mocodes', 'LOCATION', 'Cross Street'], inplace=True)

domestic_df['Vict Sex'] = domestic_df['Vict Sex'].map(sex_mapping)
domestic_df['Vict Sex'] = domestic_df['Vict Sex'].where(domestic_df['Vict Sex'].isin([0, 1]), np.nan)

domestic_df['Vict Descent'] = domestic_df['Vict Descent'].map(descent_mapping)
domestic_df = domestic_df[domestic_df['Vict Descent'] != 18]

domestic_df['Age Category'] = domestic_df['Age Category'].map(age_mapping)
domestic_df['Crm Cd'] = domestic_df['Crm Cd'].map(code_mapping)


domestic_df['Solved'] = domestic_df['Status Desc'] != 'Invest Cont'

domestic_df.dropna(inplace=True)
domestic_df.reset_index(drop=True)
domestic_df

# target variable is vict sex to see the pattern between male and female DV victims
domestic_df.reset_index(inplace=True, drop=True)

X = domestic_df[['Age Category', 'Vict Descent']]

# using as target variable bc rf from before indicates that women more at risk than men
y = domestic_df['Vict Sex']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# try forest
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)


mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

feature_importances = rf_model.feature_importances_

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.bar(range(len(feature_importances)), feature_importances, tick_label=X.columns)
plt.xlabel('Feature')
plt.ylabel('Feature Importance')
plt.title('Feature Importances for Domestic Violence')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""overwhelming pattern of 0-18 being the most vulnerable category - brings into question whether or not crimes are harder to investigate when the child is the victim. Thus --> domestic df vs crime solved. \\
mse of 0.22 \\
victim descent on top \\
"""

# dv victim vs status of crime

domestic_df.reset_index(inplace=True, drop=True)

X = domestic_df[['Age Category', 'Vict Descent', 'Vict Sex']]

y = domestic_df['Solved']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)


mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

feature_importances = rf_model.feature_importances_

plt.figure(figsize=(10, 6))
plt.bar(range(len(feature_importances)), feature_importances, tick_label=X.columns)
plt.xlabel('Feature')
plt.ylabel('Feature Importance')
plt.title('Feature Importances of Solved DV crime')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""Contrary to initial expectations, our analysis reveals that victim descent appears to have a more pronounced influence on domestic violence occurrences compared to age category. Through logistic regression analysis focused on victim descent, we aim to uncover underlying patterns in domestic violence incidents. Additionally, we set the 'solved' status to true (1) for our analysis, specifically focusing on domestic violence cases. As a reminder, our analysis is strictly confined to domestic violence incidents. It's worth noting that when we attempted to model all violent crimes, we encountered a significant challenge with low accuracy and high mean squared error (MSE)."""

# from last graph - do log reg on victim descent

X = domestic_df[['Vict Descent']]
y = domestic_df['Solved']
X = pd.get_dummies(X, columns=['Vict Descent'])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

logreg2 = LogisticRegression()
logreg2.fit(X_train, y_train)

y_pred = logreg2.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

coefficients = logreg2.coef_[0]

feature_names = X.columns
feature_coefficients2 = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})
print(feature_coefficients2)

"""curious to see if pattern exists for solved crime with property"""

solved_df = property_crimes_df.copy()

"""basically same set up as previous cases"""

solved_df['Solved'] = solved_df['Status Desc'] != 'Invest Cont'

solved_df['Vict Sex'] = solved_df['Vict Sex'].map(sex_mapping)
solved_df['Vict Sex'] = solved_df['Vict Sex'].where(solved_df['Vict Sex'].isin([0, 1]), np.nan)
solved_df.dropna(subset=['Vict Sex'], inplace=True)

solved_df['Vict Descent'] = solved_df['Vict Descent'].map(descent_mapping)
solved_df = solved_df[solved_df['Vict Descent'] != 18]
solved_df['Vict Descent'].fillna(-1, inplace=True)

solved_df['Age Category'] = solved_df['Age Category'].map(age_mapping)
# solved = true, unsolved = false
solved_df['Solved'] = solved_df['Status Desc'] != 'Invest Cont'

solved_df.drop(columns=['Crm Cd 1','Crm Cd 2', 'Crm Cd 3', 'Crm Cd 4', 'Status', 'Crm Cd Desc', 'DR_NO', 'AREA', 'AREA NAME', 'Weapon Desc',
                               'Weapon Used Cd', 'Premis Cd', 'Premis Desc', 'Mocodes', 'LOCATION', 'Cross Street'], inplace=True)
solved_df[['Age Category', 'Vict Descent', 'Vict Sex']].dropna(inplace=True)
solved_df.reset_index(inplace=True, drop=True)

"""solved vs age & descent & age (rf) \\
descent on top \\
dropped na just for rf to see if age is worth looking into (it is not) - do not drop NAs for log reg below
"""

# use rf to see which features most important
nona_solved_df = solved_df.dropna()

X = nona_solved_df[['Age Category', 'Vict Descent', 'Vict Sex']]

y = nona_solved_df['Solved']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# try forest
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error:", mse)

# Get feature importances
feature_importances = rf_model.feature_importances_

# Visualize feature importances
plt.figure(figsize=(10, 6))
plt.bar(range(len(feature_importances)), feature_importances, tick_label=X.columns)
plt.xlabel('Feature')
plt.ylabel('Feature Importance')
plt.title('Feature Importances of Solved Property Crimes')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""We observe that violent and domestic crime has different patterns than that of property crime. This was to an accuracy of 0.94"""

# solved property crimes & victim descent
X = solved_df[['Vict Descent']]
y = solved_df['Solved']
X = pd.get_dummies(X, columns=['Vict Descent'], drop_first= True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

logreg3 = LogisticRegression()
logreg3.fit(X_train, y_train)

y_pred = logreg3.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

coefficients = logreg3.coef_[0]

feature_names = X.columns
feature_coefficients3 = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})
print(feature_coefficients3)

#graph
plt.figure(figsize=(10, 6))
plt.barh(feature_coefficients3['Feature'], feature_coefficients3['Coefficient'], color='skyblue')
plt.xlabel('Coefficient Value')
plt.ylabel('Feature')
plt.title('Logistic Regression Coefficients')
plt.grid(axis='x')
plt.show()

"""**Patterns found**

- Youth Vulnerability: Youth are identified as a particularly vulnerable demographic group, with a higher likelihood of falling victim to violent crimes. This underscores the need for targeted interventions and support systems to protect and empower young individuals.
-Ethnic Disparities: Black and Hispanic individuals are disproportionately affected by violent crime, indicating significant disparities in victimization rates among different ethnic groups. This highlights systemic issues that require attention and action to address root causes and ensure equitable outcomes. \\
- Minority Vulnerability: Our findings suggest that minority populations, including Black and Hispanic communities, face heightened vulnerability to violent crime. This underscores the importance of understanding and addressing the intersecting factors contributing to their increased risk of victimization. \\
Child Sexual Assault: Children emerge as a particularly vulnerable group to sexual assault, with our analysis indicating that they are most likely to face this type of crime. This underscores the urgent need for comprehensive measures to protect children from exploitation and abuse.
"""

location_df = df.copy()
location_df = location_df[['Crm Cd', 'LAT', 'LON', 'Crm Cd Desc', 'Vict Sex', 'Vict Descent']]

"""# K-Means Clustering:
To prepare the data for clustering analysis, we first dropped the age category due to missing values. Next, we applied feature scaling using StandardScaler to normalize the latitude and longitude coordinates, aiming to ensure that both features contribute equally to the clustering process.


"""

scaler = StandardScaler()

# filters outliers
location_df = location_df[(location_df['LAT'] >= 33) & (location_df['LAT'] <= 35) &
                                    (location_df['LON'] >= -119) & (location_df['LON'] <= -117)]

scaled_location_data = scaler.fit_transform(location_df[['LAT', 'LON']])

# finding k
distortions = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(scaled_location_data)
    distortions.append(kmeans.inertia_)

plt.plot(range(2, 11), distortions, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal K')
plt.show()

"""5 clusters and average lat/long in each cluster"""

k = 5
# running k means on data with k = 5
kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
kmeans.fit(scaled_location_data)

location_df['Cluster'] = kmeans.labels_

cluster_centers = kmeans.cluster_centers_
cluster_centers_original = scaler.inverse_transform(cluster_centers)

sns.reset_orig()
plt.scatter(location_df['LON'], location_df['LAT'], c=location_df['Cluster'], cmap='viridis')
plt.scatter(cluster_centers_original[:, 1], cluster_centers_original[:, 0], c='red', marker='x', s=200, label='Cluster Centers')

plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Crime Hotspots in Los Angeles')
plt.colorbar(label='Cluster')
plt.show()

"""top crimes per cluster --> geographic patterns of crime (using SQL)"""

# top crimes per cluster
# sub queries
location_df

cluster_1_top = """
SELECT "Crm Cd", "Crm Cd Desc", COUNT(*) AS count
FROM location_df
WHERE "Cluster" == 1
GROUP BY "Crm Cd"
ORDER BY count DESC
LIMIT 5
"""

cluster_2_top = """
SELECT "Crm Cd", "Crm Cd Desc", COUNT(*) AS count
FROM location_df
WHERE "Cluster" == 2
GROUP BY "Crm Cd"
ORDER BY count DESC
LIMIT 5
"""

cluster_3_top = """
SELECT "Crm Cd", "Crm Cd Desc", COUNT(*) AS count
FROM location_df
WHERE "Cluster" == 3
GROUP BY "Crm Cd"
ORDER BY count DESC
LIMIT 5
"""

cluster_4_top = """
SELECT "Crm Cd", "Crm Cd Desc", COUNT(*) AS count
FROM location_df
WHERE "Cluster" == 4
GROUP BY "Crm Cd"
ORDER BY count DESC
LIMIT 5
"""
cluster_0_top = """
SELECT "Crm Cd", "Crm Cd Desc", COUNT(*) AS count
FROM location_df
WHERE "Cluster" == 0
GROUP BY "Crm Cd"
ORDER BY count DESC
LIMIT 5
"""

top_cluster_1_crimes = sqldf(cluster_1_top, locals())
top_cluster_2_crimes = sqldf(cluster_2_top, locals())
top_cluster_3_crimes = sqldf(cluster_3_top, locals())
top_cluster_4_crimes = sqldf(cluster_4_top, locals())
top_cluster_0_crimes = sqldf(cluster_0_top, locals())

# graphing what i just found
top_cluster_1_crimes['Cluster'] = 1
top_cluster_2_crimes['Cluster'] = 2
top_cluster_3_crimes['Cluster'] = 3
top_cluster_4_crimes['Cluster'] = 4
top_cluster_0_crimes['Cluster'] = 0

fig, axes = plt.subplots(5, 1, figsize=(10, 12), sharex=True)

axes[0].barh(top_cluster_0_crimes['Crm Cd Desc'], top_cluster_0_crimes['count'], color='r', alpha=0.7)
axes[0].set_title('Cluster 0: Top 5 Crimes')

axes[1].barh(top_cluster_1_crimes['Crm Cd Desc'], top_cluster_1_crimes['count'], color='y', alpha=0.7)
axes[1].set_title('Cluster 1: Top 5 Crimes')

axes[2].barh(top_cluster_2_crimes['Crm Cd Desc'], top_cluster_2_crimes['count'], color='g', alpha=0.7)
axes[2].set_title('Cluster 2: Top 5 Crimes')

axes[3].barh(top_cluster_3_crimes['Crm Cd Desc'], top_cluster_3_crimes['count'], color='b', alpha=0.7)
axes[3].set_title('Cluster 3: Top 5 Crimes')

axes[4].barh(top_cluster_4_crimes['Crm Cd Desc'], top_cluster_4_crimes['count'], color='m', alpha=0.7)
axes[4].set_title('Cluster 4: Top 5 Crimes')

for ax in axes:
    ax.set_xlabel('Count')
    ax.set_xlim(left=0)

plt.tight_layout()

plt.show()

gta_location_df = location_df[location_df['Crm Cd'] == 510]
gta_location_df = gta_location_df.reset_index(drop=True)

gta_location_df = location_df[location_df['Crm Cd'] == 510]

gta_location_df.drop(columns='Cluster', inplace=True)
scaled_gta_data = scaler.fit_transform(gta_location_df[['LAT', 'LON']])


# finding k
distortions = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(scaled_gta_data)
    distortions.append(kmeans.inertia_)

plt.tight_layout()

plt.plot(range(2, 11), distortions, marker='o')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal K')
plt.show()

"""regions + centers \\

then counts number of occurences in a cluster - with cluster 0 on top. let us take a closer look at cluster 0!
"""

k = 4

kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
kmeans.fit(scaled_gta_data)

gta_location_df['Cluster'] = kmeans.labels_

cluster_centers = kmeans.cluster_centers_
cluster_centers_original = scaler.inverse_transform(cluster_centers)


plt.scatter(gta_location_df['LON'], gta_location_df['LAT'], c=gta_location_df['Cluster'], cmap='viridis')
plt.scatter(cluster_centers_original[:, 1], cluster_centers_original[:, 0], c='red', marker='x', s=200, label='Cluster Centers')


plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Crime Hotspots in Los Angeles')
plt.colorbar(label='Cluster')
plt.show()

gta_location_agg_df = gta_location_df.groupby("Cluster").size().reset_index(name="Crime Count")
gta_location_agg_df

"""running same process again but JUST ON CLUSTER 0. \\
black region has most crime -- find on map where that is and whether it makes sense
"""

cluster_centers_original

# zooming in on the cluster with the most crime (cluster 0)
gta_location_0_df = gta_location_df[gta_location_df['Cluster'] == 0]

scaled_gta_data_0 = scaler.fit_transform(gta_location_0_df[['LAT', 'LON']])

k = 5

kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
kmeans.fit(scaled_gta_data_0)

gta_location_0_df['Cluster'] = kmeans.labels_

cluster_centers = kmeans.cluster_centers_
cluster_centers_original = scaler.inverse_transform(cluster_centers)

sns.reset_orig()

plt.figure(figsize=(12, 7))
scatter = plt.scatter(gta_location_0_df['LON'], gta_location_0_df['LAT'], c=gta_location_0_df['Cluster'], cmap='inferno')
plt.scatter(cluster_centers_original[:, 1], cluster_centers_original[:, 0], c='red', marker='x', s=200, label='Cluster Centers')

plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Crime Hotspots in Los Angeles')
plt.colorbar(scatter, label='Cluster')
plt.show()

gta_location_agg_df = gta_location_0_df.groupby("Cluster").size().reset_index(name="Crime Count")
gta_location_agg_df

kde = KernelDensity(bandwidth=0.01)
kde.fit(gta_location_0_df[['LAT', 'LON']])
density_scores = kde.score_samples(gta_location_0_df[['LAT', 'LON']])

plt.figure(figsize=(10, 8))
ds = plt.scatter(gta_location_0_df['LON'], gta_location_0_df['LAT'], c=density_scores, cmap='plasma')
plt.colorbar(ds, label='Density Score')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Crime Density in Cluster 0')
plt.show()

top_areas_df = df.copy()
unique_areas = top_areas_df.groupby('AREA')['AREA NAME'].unique()
area_counts = top_areas_df['AREA'].value_counts().reset_index()
area_counts.columns = ['AREA', 'count']


unique_values = df.groupby('Crm Cd')['Crm Cd Desc'].unique()
crime_counts = df['Crm Cd'].value_counts().reset_index()

unique_areas = pd.merge(unique_areas, area_counts, on='AREA', how='left').sort_values(by='count', ascending=False)
unique_areas.head(5).reset_index(drop=True)

cluster_centers_original

"""**Insights and Limitations Documentation:**

**Insight 1:**
The intensified policing of inner city communities post-protests may have contributed to a surge in reported crimes, thereby inflating the overall crime statistics.

Insight Explanation:
Following the BLM protests, there was a notable increase in law enforcement presence in inner city areas. This heightened policing may have led to an increase in the reporting of crimes, potentially inflating overall crime statistics. The surge in reported crimes could be attributed to heightened vigilance and law enforcement activity rather than an actual increase in criminal behavior.

Limitations:
1. **Underreporting Bias:** The reliance on reported crime data may introduce bias, as not all crimes are reported to law enforcement. This could lead to an incomplete understanding of the true extent of criminal activity.
2. **Causation vs. Correlation:** While there is a correlation between intensified policing and increased crime reports, establishing causation requires further investigation. Other factors, such as community trust in law enforcement and changes in reporting behavior, could also influence crime statistics.

**Insight 2:**
The aftermath of the protests might have fueled heightened civil unrest, leading to increased criminal activities, community instability, and a rise in theft incidents.

Insight Explanation:
The BLM protests and their aftermath may have contributed to heightened civil unrest, potentially leading to increased criminal activities. This could include acts of theft and vandalism, driven by community instability and tensions surrounding social justice issues.

Limitations:
1. **Complexity of Social Dynamics:** While there may be a correlation between the protests and increased criminal activities, understanding the precise causal relationship requires consideration of various social, economic, and political factors.
2. **Temporal Analysis:** The immediate aftermath of the protests may show a spike in criminal activities, but the long-term effects on crime rates require further monitoring and analysis.

**Insight 3:**
The current spike in crime could be a transient phenomenon linked to challenging economic circumstances experienced by many, suggesting a temporary uptick rather than a sustained trend.

Insight Explanation:
The current spike in crime rates may be transient, influenced by challenging economic conditions experienced by many individuals. Economic hardship can contribute to increased desperation and criminal behavior, potentially leading to a temporary uptick in crime rates.

Limitations:
1. **Economic Indicators:** While economic circumstances may influence crime rates, other factors such as policing strategies and social dynamics also play significant roles. A comprehensive analysis should consider multiple variables to understand the complexities of crime trends.
2. **Long-term Trends:** While the spike in crime rates may be temporary, sustained monitoring and analysis are necessary to determine if this trend persists or if it fluctuates over time.

**Conclusion:**
The insights provided highlight potential factors contributing to changes in crime rates, including intensified policing, civil unrest, and economic conditions. However, these insights are subject to limitations such as underreporting bias, the complexity of social dynamics, and the need for long-term analysis. Further research and data analysis are necessary to develop a comprehensive understanding of crime trends and their underlying drivers.

"""